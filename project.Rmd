---
title: "R Notebook"
output: html_notebook
---

# Packages and Inputs

```{r libraries in use}
library(xgboost) # for xgboost
library(tidyverse) # general utility functions
```

```{r read in our data & put it in a data frame}
diseaseInfo <- read_csv("C:/Users/User/Desktop/university/machine learning/machine learning - salini/dataset/Outbreak_240817.csv")
```

# Preparing our data & selecting features

The core xgboost function requires data to be a matrix.
A matrix is like a dataframe that only has numbers in it. A sparse matrix is a matrix that has a lot zeros in it. XGBoost has a built-in datatype, DMatrix, that is particularly good at storing and accessing sparse matrices efficiently.

```{r print the first few rows of our dataframe}
head(diseaseInfo)
```

our data will need some cleaning before it's ready to be put in a matrix. To prepare our data, we have a number of steps we need to complete:

- Remove information about the target variable from the training data
- Reduce the amount of redundant information
- Convert categorical information (like country) to a numeric format
- Split dataset into testing and training subsets
- Convert the cleaned dataframe to a Dmatrix

## Remove information about the target variable from the training data

```{r remove the columns that have information on our target variable}
diseaseInfo_humansRemoved <- diseaseInfo %>% select(-starts_with("human")) # get the subset of the dataframe that doesn't have labels about humans affected by the disease
```

Let's create a new vector with the labels

```{r get a boolean vector of training labels}
diseaseLabels <- diseaseInfo %>% 
  select(humansAffected) %>% # get the column with the # of humans affected
  is.na() %>% # is it NA?
  magrittr::not() # switch TRUE and FALSE (using function from the magrittr package)

# check out the first few lines
head(diseaseLabels) # of our target variable
head(diseaseInfo$humansAffected) # of the original column
```

## Reduce the amount of redundant information

```{r select just the numeric columns}
diseaseInfo_numeric <- diseaseInfo_humansRemoved %>%
    select(-Id) %>% # the case id shouldn't contain useful information
    select(-c(longitude, latitude)) %>% # location data is also in country data
    select_if(is.numeric) # select remaining numeric columns

# make sure that our dataframe is all numeric
str(diseaseInfo_numeric)
```

## Convert categorical information (like country) to a numeric format

```{r check out the first few rows of the country column}
head(diseaseInfo$country)
```

```{r convert these categories to a matrix}
model.matrix(~country-1,head(diseaseInfo)) # one-hot matrix for just the first few rows of the "country" column
```

```{r convert categorical factor into one-hot encoding}
region <- model.matrix(~country-1,diseaseInfo)

# check out the first few lines of the species
head(diseaseInfo$speciesDescription)
```

```{r add a boolean column to our numeric dataframe indicating whether a species is domestic}

diseaseInfo_numeric$is_domestic <- str_detect(diseaseInfo$speciesDescription, "domestic")
```

```{r create a one-hot matrix of different species}
# grab the last word of each row and use that to create a one-hot matrix of different species

# get a list of all the species by getting the last
speciesList <- diseaseInfo$speciesDescription %>%
    str_replace("[[:punct:]]", "") %>% # remove punctuation (some rows have parentheses)
    str_extract("[a-z]*$") # extract the least word in each row

# convert our list into a dataframe...
speciesList <- tibble(species = speciesList)

# and convert to a matrix using 1 hot encoding
options(na.action='na.pass') # don't drop NA values!
species <- model.matrix(~species-1,speciesList)

# add our one-hot encoded variable and convert the dataframe into a matrix
diseaseInfo_numeric <- cbind(diseaseInfo_numeric, region, species)
diseaseInfo_matrix <- data.matrix(diseaseInfo_numeric)
```

## Split the dataset to model

```{r Split dataset into testing and training subsets}
# get the numb 70/30 training test split
numberOfTrainingSamples <- round(length(diseaseLabels) * .7)

# training data
train_data <- diseaseInfo_matrix[1:numberOfTrainingSamples,]
train_labels <- diseaseLabels[1:numberOfTrainingSamples]

# testing data
test_data <- diseaseInfo_matrix[-(1:numberOfTrainingSamples),]
test_labels <- diseaseLabels[-(1:numberOfTrainingSamples)]
```

```{r Convert the cleaned dataframe to a dmatrix}
# put our testing & training data into two seperates Dmatrixs objects
dtrain <- xgb.DMatrix(data = train_data, label= train_labels)
dtest <- xgb.DMatrix(data = test_data, label= test_labels)
```


# Analysis

```{r set a random seed & shuffle data frame}
set.seed(1234)
diseaseInfo <- diseaseInfo[sample(1:nrow(diseaseInfo)), ]
```

```{r train a model using our training data}
model <- xgboost(data = dtrain, # the data   
                 nround = 2, # max number of boosting iterations
                 objective = "binary:logistic")  # the objective function
```

```{r make prediction}
# generate predictions for our held-out testing data
pred <- predict(model, dtest)

# get & print the classification error
err <- mean(as.numeric(pred > 0.5) != test_labels)
print(paste("test-error=", err))
```
## Tuninn our Model

```{r}
# train an xgboost model
model_tuned <- xgboost(data = dtrain, # the data           
                 max.depth = 3, # the maximum depth of each decision tree
                 nround = 2, # max number of boosting iterations
                 objective = "binary:logistic") # the objective function 

# generate predictions for our held-out testing data
pred <- predict(model_tuned, dtest)

# get & print the classification error
err <- mean(as.numeric(pred > 0.5) != test_labels)
print(paste("test-error=", err))
```

There are two things we can try to see if we improve our model performance:
- Account for the fact that we have imbalanced classes
- Train for more rounds

```{r re-training our model}
# get the number of negative & positive cases in our data
negative_cases <- sum(train_labels == FALSE)
postive_cases <- sum(train_labels == TRUE)

# train a model using our training data
model_tuned <- xgboost(data = dtrain, # the data           
                 max.depth = 3, # the maximum depth of each decision tree
                 nround = 10, # number of boosting rounds
                 early_stopping_rounds = 3, # if we dont see an improvement in this many rounds, stop
                 objective = "binary:logistic", # the objective function
                 scale_pos_weight = negative_cases/postive_cases) # control for imbalanced classes

# generate predictions for our held-out testing data
pred <- predict(model_tuned, dtest)

# get & print the classification error
err <- mean(as.numeric(pred > 0.5) != test_labels)
print(paste("test-error=", err))
```

... TODO

```{r }
# train a model using our training data
model_tuned <- xgboost(data = dtrain, # the data           
                 max.depth = 3, # the maximum depth of each decision tree
                 nround = 10, # number of boosting rounds
                 early_stopping_rounds = 3, # if we dont see an improvement in this many rounds, stop
                 objective = "binary:logistic", # the objective function
                 scale_pos_weight = negative_cases/postive_cases, # control for imbalanced classes
                 gamma = 1) # add a regularization term

# generate predictions for our held-out testing data
pred <- predict(model_tuned, dtest)

# get & print the classification error
err <- mean(as.numeric(pred > 0.5) != test_labels)
print(paste("test-error=", err))
```

## Interpretation

```{r }
# plot them features! what's contributing most to our model?
xgb.plot.multi.trees(feature_names = names(diseaseInfo_matrix), 
                     model = model)
```

Because we're using a logistic model here, it's telling us the log-odds rather than the probability

```{r}
# convert log odds to probability
odds_to_probs <- function(odds){
    return(exp(odds)/ (1 + exp(odds)))
}


# probability of leaf above countryPortugul
odds_to_probs(-0.599)
```

```{r plotting the importance matrix}
# get information on how important each feature is
importance_matrix <- xgb.importance(names(diseaseInfo_matrix), model = model)

# and plot it!
xgb.plot.importance(importance_matrix)

```

